{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "instanceSegmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "784J9uLrmx84",
        "outputId": "dbfdaa35-2ea6-4cd8-8a69-44ba2cf516ef"
      },
      "source": [
        "%cd /content/drive/MyDrive/DL_HW3"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DL_HW3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYr3ibFUnh0H"
      },
      "source": [
        "!unzip test_images.zip -d /content/drive/MyDrive/DL_HW3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVMWhuvLnmG6"
      },
      "source": [
        "!unzip train_images.zip -d /content/drive/MyDrive/DL_HW3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhC1tEx2oeCe",
        "outputId": "50d46f16-9a4c-47d1-d557-056491d04528"
      },
      "source": [
        "!pip install -U torch torchvision\n",
        "!pip install git+https://github.com/facebookresearch/fvcore.git\n",
        "#!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
        "!pip install -e detectron2_repo"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already up-to-date: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (8.0.1)\n",
            "Collecting git+https://github.com/facebookresearch/fvcore.git\n",
            "  Cloning https://github.com/facebookresearch/fvcore.git to /tmp/pip-req-build-jvnjhodr\n",
            "  Running command git clone -q https://github.com/facebookresearch/fvcore.git /tmp/pip-req-build-jvnjhodr\n",
            "Requirement already satisfied (use --upgrade to upgrade): fvcore==0.1.2 from git+https://github.com/facebookresearch/fvcore.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (1.18.5)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (5.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (4.41.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (2.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (8.0.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (0.8.7)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.2-cp36-none-any.whl size=48498 sha256=8dd721fb5d9f579cf712716c7f4a3dd5f4d34250cd894ee7fdf28ac95b6693d6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-notn13vv/wheels/48/53/79/3c6485543a4455a0006f5db590ab9957622b6227011941de06\n",
            "Successfully built fvcore\n",
            "Obtaining file:///content/drive/MyDrive/DL_HW3/detectron2_repo\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.3) (1.1.0)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.3) (8.0.1)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.3) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2==0.3) (0.8.7)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2==0.3) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2==0.3) (3.2.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.3) (4.41.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2==0.3) (2.3.0)\n",
            "Requirement already satisfied: fvcore>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.3) (0.1.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.3) (2.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from detectron2==0.3) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (from detectron2==0.3) (1.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from yacs>=0.1.6->detectron2==0.3) (5.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.3) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.3) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.3) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.3) (1.18.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.3) (1.3.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.3) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.3) (1.17.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.3) (0.10.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.3) (1.33.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.3) (50.3.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.3) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.3) (1.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.3) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.3) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.3) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.3) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.3) (0.35.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from fvcore>=0.1.2->detectron2==0.3) (2.0.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools>=2.0.2->detectron2==0.3) (0.29.21)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.3) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.3) (3.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.3) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.3) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.3) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.3) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.3) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2==0.3) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2==0.3) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.3) (3.1.0)\n",
            "Installing collected packages: detectron2\n",
            "  Found existing installation: detectron2 0.3\n",
            "    Can't uninstall 'detectron2'. No files were found to uninstall.\n",
            "  Running setup.py develop for detectron2\n",
            "Successfully installed detectron2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbRF8TAtrSPC",
        "outputId": "e31e3026-0a77-4666-9915-4a06bf3d07a1"
      },
      "source": [
        "%cd /content/drive/MyDrive/DL_HW3/detectron2_repo"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DL_HW3/detectron2_repo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvjlCVW0pl73"
      },
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"Tiny_dataset\", {}, \"/content/drive/MyDrive/DL_HW3/pascal_train.json\", \"/content/drive/MyDrive/DL_HW3/train_images\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tlyu1BurZwV"
      },
      "source": [
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "Tiny_metadata = MetadataCatalog.get(\"Tiny_dataset\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cCHR549sXWL",
        "outputId": "f0c6981f-a97a-4702-b9e4-c87329326fa0"
      },
      "source": [
        "Tiny_metadata"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Metadata(evaluator_type='coco', image_root='/content/drive/MyDrive/DL_HW3/train_images', json_file='/content/drive/MyDrive/DL_HW3/pascal_train.json', name='Tiny_dataset')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hGIr2HQs1nC",
        "outputId": "f6436e61-01ee-45e7-b7b2-e19df2dabb40"
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "import os\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"./configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")\n",
        "cfg.DATASETS.TRAIN = (\"Tiny_dataset\",)\n",
        "cfg.DATASETS.TEST = ()  \n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = \"detectron2://ImageNetPretrained/MSRA/R-101.pkl\"  # initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.0001\n",
        "cfg.SOLVER.MAX_ITER = (\n",
        "    15000\n",
        ")  # 300 iterations seems good enough, but you can certainly train longer\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = (\n",
        "    128\n",
        ")  # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 20  \n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)  # If want resume training, set resume = true\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[12/02 14:36:13 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (6): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (7): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (8): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (9): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (10): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (11): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (12): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (13): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (14): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (15): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (16): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (17): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (18): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (19): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (20): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (21): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (22): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=21, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=80, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[12/02 14:36:13 d2.data.datasets.coco]: \u001b[0mLoaded 1349 images in COCO format from /content/drive/MyDrive/DL_HW3/pascal_train.json\n",
            "\u001b[32m[12/02 14:36:13 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 1349 images left.\n",
            "\u001b[32m[12/02 14:36:13 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[12/02 14:36:13 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[12/02 14:36:13 d2.data.common]: \u001b[0mSerializing 1349 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[12/02 14:36:13 d2.data.common]: \u001b[0mSerialized dataset takes 5.04 MiB\n",
            "\u001b[32m[12/02 14:36:31 d2.engine.train_loop]: \u001b[0mStarting training from iteration 10000\n",
            "\u001b[32m[12/02 14:36:45 d2.utils.events]: \u001b[0m eta: 0:52:53  iter: 10019  total_loss: 0.8983  loss_cls: 0.1569  loss_box_reg: 0.4151  loss_mask: 0.3141  loss_rpn_cls: 0.0142  loss_rpn_loc: 0.01791  time: 0.6403  data_time: 0.0436  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:36:59 d2.utils.events]: \u001b[0m eta: 0:54:34  iter: 10039  total_loss: 1.121  loss_cls: 0.1947  loss_box_reg: 0.4792  loss_mask: 0.3436  loss_rpn_cls: 0.01636  loss_rpn_loc: 0.03509  time: 0.6753  data_time: 0.0058  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:37:13 d2.utils.events]: \u001b[0m eta: 0:55:37  iter: 10059  total_loss: 1.177  loss_cls: 0.2721  loss_box_reg: 0.5212  loss_mask: 0.3583  loss_rpn_cls: 0.01888  loss_rpn_loc: 0.03163  time: 0.6753  data_time: 0.0075  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:37:27 d2.utils.events]: \u001b[0m eta: 0:55:45  iter: 10079  total_loss: 0.9717  loss_cls: 0.1652  loss_box_reg: 0.4815  loss_mask: 0.2929  loss_rpn_cls: 0.01397  loss_rpn_loc: 0.01556  time: 0.6815  data_time: 0.0068  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:37:41 d2.utils.events]: \u001b[0m eta: 0:55:36  iter: 10099  total_loss: 1.154  loss_cls: 0.2147  loss_box_reg: 0.4897  loss_mask: 0.3444  loss_rpn_cls: 0.01306  loss_rpn_loc: 0.01666  time: 0.6822  data_time: 0.0292  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:37:55 d2.utils.events]: \u001b[0m eta: 0:56:10  iter: 10119  total_loss: 1.101  loss_cls: 0.1987  loss_box_reg: 0.4955  loss_mask: 0.3754  loss_rpn_cls: 0.02078  loss_rpn_loc: 0.02332  time: 0.6870  data_time: 0.0057  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:38:09 d2.utils.events]: \u001b[0m eta: 0:55:47  iter: 10139  total_loss: 0.8326  loss_cls: 0.1524  loss_box_reg: 0.3356  loss_mask: 0.297  loss_rpn_cls: 0.01354  loss_rpn_loc: 0.01705  time: 0.6872  data_time: 0.0057  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:38:22 d2.utils.events]: \u001b[0m eta: 0:55:26  iter: 10159  total_loss: 0.9701  loss_cls: 0.1955  loss_box_reg: 0.4653  loss_mask: 0.3038  loss_rpn_cls: 0.01214  loss_rpn_loc: 0.01625  time: 0.6877  data_time: 0.0309  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:38:37 d2.utils.events]: \u001b[0m eta: 0:55:24  iter: 10179  total_loss: 1.094  loss_cls: 0.1756  loss_box_reg: 0.4469  loss_mask: 0.3692  loss_rpn_cls: 0.01682  loss_rpn_loc: 0.0201  time: 0.6903  data_time: 0.0051  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:38:51 d2.utils.events]: \u001b[0m eta: 0:55:16  iter: 10199  total_loss: 1.154  loss_cls: 0.2133  loss_box_reg: 0.4979  loss_mask: 0.3525  loss_rpn_cls: 0.02695  loss_rpn_loc: 0.02266  time: 0.6913  data_time: 0.0055  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:39:05 d2.utils.events]: \u001b[0m eta: 0:54:59  iter: 10219  total_loss: 1.072  loss_cls: 0.1884  loss_box_reg: 0.5094  loss_mask: 0.3233  loss_rpn_cls: 0.01742  loss_rpn_loc: 0.01997  time: 0.6915  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:39:19 d2.utils.events]: \u001b[0m eta: 0:54:46  iter: 10239  total_loss: 1.128  loss_cls: 0.2313  loss_box_reg: 0.4766  loss_mask: 0.3343  loss_rpn_cls: 0.01596  loss_rpn_loc: 0.02428  time: 0.6918  data_time: 0.0240  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:39:33 d2.utils.events]: \u001b[0m eta: 0:54:45  iter: 10259  total_loss: 1.064  loss_cls: 0.2201  loss_box_reg: 0.4243  loss_mask: 0.3254  loss_rpn_cls: 0.0188  loss_rpn_loc: 0.02461  time: 0.6954  data_time: 0.0307  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:39:47 d2.utils.events]: \u001b[0m eta: 0:54:30  iter: 10279  total_loss: 1.034  loss_cls: 0.218  loss_box_reg: 0.4599  loss_mask: 0.3413  loss_rpn_cls: 0.02045  loss_rpn_loc: 0.02052  time: 0.6951  data_time: 0.0055  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:40:01 d2.utils.events]: \u001b[0m eta: 0:54:16  iter: 10299  total_loss: 0.967  loss_cls: 0.1849  loss_box_reg: 0.4503  loss_mask: 0.3089  loss_rpn_cls: 0.02196  loss_rpn_loc: 0.02001  time: 0.6943  data_time: 0.0053  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:40:15 d2.utils.events]: \u001b[0m eta: 0:54:10  iter: 10319  total_loss: 0.927  loss_cls: 0.1529  loss_box_reg: 0.3696  loss_mask: 0.2798  loss_rpn_cls: 0.01792  loss_rpn_loc: 0.01508  time: 0.6951  data_time: 0.0052  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:40:29 d2.utils.events]: \u001b[0m eta: 0:53:56  iter: 10339  total_loss: 1.184  loss_cls: 0.2138  loss_box_reg: 0.5326  loss_mask: 0.3829  loss_rpn_cls: 0.02145  loss_rpn_loc: 0.01753  time: 0.6957  data_time: 0.0062  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:40:44 d2.utils.events]: \u001b[0m eta: 0:53:51  iter: 10359  total_loss: 1.204  loss_cls: 0.2173  loss_box_reg: 0.5167  loss_mask: 0.3471  loss_rpn_cls: 0.0236  loss_rpn_loc: 0.02761  time: 0.6975  data_time: 0.0057  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:40:57 d2.utils.events]: \u001b[0m eta: 0:53:37  iter: 10379  total_loss: 0.8715  loss_cls: 0.1475  loss_box_reg: 0.3991  loss_mask: 0.2709  loss_rpn_cls: 0.01221  loss_rpn_loc: 0.0107  time: 0.6969  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:41:11 d2.utils.events]: \u001b[0m eta: 0:53:16  iter: 10399  total_loss: 1.241  loss_cls: 0.2586  loss_box_reg: 0.4891  loss_mask: 0.3506  loss_rpn_cls: 0.02367  loss_rpn_loc: 0.02707  time: 0.6954  data_time: 0.0060  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:41:25 d2.utils.events]: \u001b[0m eta: 0:53:05  iter: 10419  total_loss: 1.07  loss_cls: 0.2023  loss_box_reg: 0.5088  loss_mask: 0.3486  loss_rpn_cls: 0.01876  loss_rpn_loc: 0.01685  time: 0.6956  data_time: 0.0205  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:41:38 d2.utils.events]: \u001b[0m eta: 0:52:46  iter: 10439  total_loss: 1.032  loss_cls: 0.1836  loss_box_reg: 0.4564  loss_mask: 0.341  loss_rpn_cls: 0.01728  loss_rpn_loc: 0.02297  time: 0.6943  data_time: 0.0054  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:41:52 d2.utils.events]: \u001b[0m eta: 0:52:35  iter: 10459  total_loss: 0.9485  loss_cls: 0.175  loss_box_reg: 0.4592  loss_mask: 0.3123  loss_rpn_cls: 0.01312  loss_rpn_loc: 0.02257  time: 0.6948  data_time: 0.0052  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:42:06 d2.utils.events]: \u001b[0m eta: 0:52:21  iter: 10479  total_loss: 1.166  loss_cls: 0.221  loss_box_reg: 0.4981  loss_mask: 0.357  loss_rpn_cls: 0.01869  loss_rpn_loc: 0.02539  time: 0.6945  data_time: 0.0053  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:42:20 d2.utils.events]: \u001b[0m eta: 0:52:10  iter: 10499  total_loss: 1.088  loss_cls: 0.258  loss_box_reg: 0.4115  loss_mask: 0.3001  loss_rpn_cls: 0.02037  loss_rpn_loc: 0.02303  time: 0.6952  data_time: 0.0055  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:42:34 d2.utils.events]: \u001b[0m eta: 0:51:56  iter: 10519  total_loss: 0.8693  loss_cls: 0.1512  loss_box_reg: 0.3536  loss_mask: 0.3189  loss_rpn_cls: 0.01798  loss_rpn_loc: 0.01595  time: 0.6956  data_time: 0.0050  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:42:48 d2.utils.events]: \u001b[0m eta: 0:51:40  iter: 10539  total_loss: 1.055  loss_cls: 0.1772  loss_box_reg: 0.4659  loss_mask: 0.3477  loss_rpn_cls: 0.01569  loss_rpn_loc: 0.0178  time: 0.6955  data_time: 0.0088  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:43:02 d2.utils.events]: \u001b[0m eta: 0:51:28  iter: 10559  total_loss: 1.191  loss_cls: 0.2179  loss_box_reg: 0.4872  loss_mask: 0.3591  loss_rpn_cls: 0.0169  loss_rpn_loc: 0.02167  time: 0.6956  data_time: 0.0051  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:43:16 d2.utils.events]: \u001b[0m eta: 0:51:14  iter: 10579  total_loss: 1.119  loss_cls: 0.19  loss_box_reg: 0.5305  loss_mask: 0.3351  loss_rpn_cls: 0.02113  loss_rpn_loc: 0.02093  time: 0.6955  data_time: 0.0053  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:43:30 d2.utils.events]: \u001b[0m eta: 0:50:58  iter: 10599  total_loss: 0.9967  loss_cls: 0.1828  loss_box_reg: 0.4681  loss_mask: 0.3423  loss_rpn_cls: 0.02365  loss_rpn_loc: 0.0165  time: 0.6956  data_time: 0.0073  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:43:44 d2.utils.events]: \u001b[0m eta: 0:50:43  iter: 10619  total_loss: 0.8918  loss_cls: 0.1829  loss_box_reg: 0.3845  loss_mask: 0.3117  loss_rpn_cls: 0.01748  loss_rpn_loc: 0.01665  time: 0.6955  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:43:57 d2.utils.events]: \u001b[0m eta: 0:50:24  iter: 10639  total_loss: 1.087  loss_cls: 0.1862  loss_box_reg: 0.4934  loss_mask: 0.3184  loss_rpn_cls: 0.02072  loss_rpn_loc: 0.01774  time: 0.6948  data_time: 0.0052  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:44:12 d2.utils.events]: \u001b[0m eta: 0:50:14  iter: 10659  total_loss: 1.055  loss_cls: 0.2147  loss_box_reg: 0.4559  loss_mask: 0.3403  loss_rpn_cls: 0.02036  loss_rpn_loc: 0.01978  time: 0.6955  data_time: 0.0053  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:44:25 d2.utils.events]: \u001b[0m eta: 0:49:54  iter: 10679  total_loss: 1.143  loss_cls: 0.2286  loss_box_reg: 0.5314  loss_mask: 0.3112  loss_rpn_cls: 0.01458  loss_rpn_loc: 0.02084  time: 0.6949  data_time: 0.0223  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:44:39 d2.utils.events]: \u001b[0m eta: 0:49:40  iter: 10699  total_loss: 1.128  loss_cls: 0.1699  loss_box_reg: 0.4991  loss_mask: 0.3437  loss_rpn_cls: 0.01882  loss_rpn_loc: 0.01774  time: 0.6947  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:44:53 d2.utils.events]: \u001b[0m eta: 0:49:29  iter: 10719  total_loss: 0.9911  loss_cls: 0.1713  loss_box_reg: 0.4057  loss_mask: 0.3151  loss_rpn_cls: 0.01427  loss_rpn_loc: 0.01925  time: 0.6951  data_time: 0.0055  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:45:07 d2.utils.events]: \u001b[0m eta: 0:49:19  iter: 10739  total_loss: 0.9815  loss_cls: 0.1824  loss_box_reg: 0.4806  loss_mask: 0.3272  loss_rpn_cls: 0.01514  loss_rpn_loc: 0.02079  time: 0.6954  data_time: 0.0060  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:45:21 d2.utils.events]: \u001b[0m eta: 0:48:58  iter: 10759  total_loss: 0.9493  loss_cls: 0.144  loss_box_reg: 0.4169  loss_mask: 0.338  loss_rpn_cls: 0.01558  loss_rpn_loc: 0.01892  time: 0.6945  data_time: 0.0055  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:45:34 d2.utils.events]: \u001b[0m eta: 0:48:39  iter: 10779  total_loss: 1.006  loss_cls: 0.1934  loss_box_reg: 0.4251  loss_mask: 0.2945  loss_rpn_cls: 0.01312  loss_rpn_loc: 0.01688  time: 0.6933  data_time: 0.0051  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:45:47 d2.utils.events]: \u001b[0m eta: 0:48:25  iter: 10799  total_loss: 1.062  loss_cls: 0.1952  loss_box_reg: 0.4946  loss_mask: 0.3484  loss_rpn_cls: 0.01739  loss_rpn_loc: 0.01855  time: 0.6931  data_time: 0.0062  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:46:01 d2.utils.events]: \u001b[0m eta: 0:48:09  iter: 10819  total_loss: 1.187  loss_cls: 0.2158  loss_box_reg: 0.4822  loss_mask: 0.3772  loss_rpn_cls: 0.01888  loss_rpn_loc: 0.03159  time: 0.6929  data_time: 0.0072  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:46:15 d2.utils.events]: \u001b[0m eta: 0:47:54  iter: 10839  total_loss: 0.8722  loss_cls: 0.1912  loss_box_reg: 0.4007  loss_mask: 0.2865  loss_rpn_cls: 0.02375  loss_rpn_loc: 0.01614  time: 0.6927  data_time: 0.0068  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:46:29 d2.utils.events]: \u001b[0m eta: 0:47:44  iter: 10859  total_loss: 1.059  loss_cls: 0.2134  loss_box_reg: 0.4853  loss_mask: 0.3174  loss_rpn_cls: 0.01578  loss_rpn_loc: 0.01565  time: 0.6931  data_time: 0.0053  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:46:42 d2.utils.events]: \u001b[0m eta: 0:47:27  iter: 10879  total_loss: 1.065  loss_cls: 0.1932  loss_box_reg: 0.4784  loss_mask: 0.3373  loss_rpn_cls: 0.01923  loss_rpn_loc: 0.01281  time: 0.6926  data_time: 0.0072  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:46:56 d2.utils.events]: \u001b[0m eta: 0:47:13  iter: 10899  total_loss: 1.032  loss_cls: 0.1856  loss_box_reg: 0.4573  loss_mask: 0.309  loss_rpn_cls: 0.01765  loss_rpn_loc: 0.02232  time: 0.6925  data_time: 0.0052  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:47:10 d2.utils.events]: \u001b[0m eta: 0:47:05  iter: 10919  total_loss: 1.061  loss_cls: 0.2136  loss_box_reg: 0.4778  loss_mask: 0.3084  loss_rpn_cls: 0.01304  loss_rpn_loc: 0.01967  time: 0.6929  data_time: 0.0066  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:47:24 d2.utils.events]: \u001b[0m eta: 0:46:51  iter: 10939  total_loss: 1.013  loss_cls: 0.2121  loss_box_reg: 0.4577  loss_mask: 0.2982  loss_rpn_cls: 0.01572  loss_rpn_loc: 0.01997  time: 0.6929  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:47:38 d2.utils.events]: \u001b[0m eta: 0:46:36  iter: 10959  total_loss: 0.9555  loss_cls: 0.1551  loss_box_reg: 0.416  loss_mask: 0.3431  loss_rpn_cls: 0.01209  loss_rpn_loc: 0.01921  time: 0.6925  data_time: 0.0049  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:47:51 d2.utils.events]: \u001b[0m eta: 0:46:21  iter: 10979  total_loss: 1.11  loss_cls: 0.208  loss_box_reg: 0.5082  loss_mask: 0.3417  loss_rpn_cls: 0.01478  loss_rpn_loc: 0.01753  time: 0.6920  data_time: 0.0065  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:48:05 d2.utils.events]: \u001b[0m eta: 0:46:06  iter: 10999  total_loss: 1.041  loss_cls: 0.1722  loss_box_reg: 0.4514  loss_mask: 0.3413  loss_rpn_cls: 0.01338  loss_rpn_loc: 0.02118  time: 0.6918  data_time: 0.0054  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:48:18 d2.utils.events]: \u001b[0m eta: 0:45:55  iter: 11019  total_loss: 1.251  loss_cls: 0.2319  loss_box_reg: 0.5466  loss_mask: 0.3775  loss_rpn_cls: 0.018  loss_rpn_loc: 0.02555  time: 0.6914  data_time: 0.0054  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:48:32 d2.utils.events]: \u001b[0m eta: 0:45:39  iter: 11039  total_loss: 0.8554  loss_cls: 0.1288  loss_box_reg: 0.3959  loss_mask: 0.2772  loss_rpn_cls: 0.01175  loss_rpn_loc: 0.01311  time: 0.6912  data_time: 0.0074  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:48:46 d2.utils.events]: \u001b[0m eta: 0:45:26  iter: 11059  total_loss: 1.173  loss_cls: 0.1996  loss_box_reg: 0.4833  loss_mask: 0.3764  loss_rpn_cls: 0.01991  loss_rpn_loc: 0.03445  time: 0.6913  data_time: 0.0061  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:49:00 d2.utils.events]: \u001b[0m eta: 0:45:13  iter: 11079  total_loss: 0.9527  loss_cls: 0.1758  loss_box_reg: 0.4013  loss_mask: 0.3227  loss_rpn_cls: 0.02003  loss_rpn_loc: 0.01805  time: 0.6915  data_time: 0.0061  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:49:13 d2.utils.events]: \u001b[0m eta: 0:45:00  iter: 11099  total_loss: 1.14  loss_cls: 0.2  loss_box_reg: 0.4271  loss_mask: 0.3608  loss_rpn_cls: 0.02092  loss_rpn_loc: 0.02801  time: 0.6913  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:49:27 d2.utils.events]: \u001b[0m eta: 0:44:40  iter: 11119  total_loss: 1.151  loss_cls: 0.204  loss_box_reg: 0.5321  loss_mask: 0.3401  loss_rpn_cls: 0.01637  loss_rpn_loc: 0.0225  time: 0.6909  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:49:40 d2.utils.events]: \u001b[0m eta: 0:44:25  iter: 11139  total_loss: 1.01  loss_cls: 0.1898  loss_box_reg: 0.4319  loss_mask: 0.3246  loss_rpn_cls: 0.01908  loss_rpn_loc: 0.01787  time: 0.6903  data_time: 0.0053  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:49:54 d2.utils.events]: \u001b[0m eta: 0:44:12  iter: 11159  total_loss: 0.969  loss_cls: 0.1814  loss_box_reg: 0.4589  loss_mask: 0.288  loss_rpn_cls: 0.01346  loss_rpn_loc: 0.023  time: 0.6903  data_time: 0.0051  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:50:08 d2.utils.events]: \u001b[0m eta: 0:43:59  iter: 11179  total_loss: 0.8894  loss_cls: 0.1496  loss_box_reg: 0.4124  loss_mask: 0.2892  loss_rpn_cls: 0.01885  loss_rpn_loc: 0.02449  time: 0.6902  data_time: 0.0057  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:50:22 d2.utils.events]: \u001b[0m eta: 0:43:47  iter: 11199  total_loss: 1.114  loss_cls: 0.2443  loss_box_reg: 0.5073  loss_mask: 0.2913  loss_rpn_cls: 0.01363  loss_rpn_loc: 0.02317  time: 0.6906  data_time: 0.0055  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:50:36 d2.utils.events]: \u001b[0m eta: 0:43:35  iter: 11219  total_loss: 1.032  loss_cls: 0.2085  loss_box_reg: 0.4471  loss_mask: 0.2995  loss_rpn_cls: 0.01602  loss_rpn_loc: 0.02101  time: 0.6907  data_time: 0.0061  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:50:49 d2.utils.events]: \u001b[0m eta: 0:43:18  iter: 11239  total_loss: 1.016  loss_cls: 0.1967  loss_box_reg: 0.45  loss_mask: 0.3414  loss_rpn_cls: 0.01766  loss_rpn_loc: 0.0191  time: 0.6904  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:51:03 d2.utils.events]: \u001b[0m eta: 0:42:58  iter: 11259  total_loss: 1.004  loss_cls: 0.1909  loss_box_reg: 0.4646  loss_mask: 0.3107  loss_rpn_cls: 0.01704  loss_rpn_loc: 0.02315  time: 0.6906  data_time: 0.0067  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:51:17 d2.utils.events]: \u001b[0m eta: 0:42:50  iter: 11279  total_loss: 0.9096  loss_cls: 0.1779  loss_box_reg: 0.403  loss_mask: 0.2752  loss_rpn_cls: 0.01608  loss_rpn_loc: 0.01688  time: 0.6907  data_time: 0.0066  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:51:31 d2.utils.events]: \u001b[0m eta: 0:42:35  iter: 11299  total_loss: 0.8591  loss_cls: 0.1548  loss_box_reg: 0.3663  loss_mask: 0.2863  loss_rpn_cls: 0.009414  loss_rpn_loc: 0.01458  time: 0.6906  data_time: 0.0052  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:51:45 d2.utils.events]: \u001b[0m eta: 0:42:14  iter: 11319  total_loss: 0.8425  loss_cls: 0.1418  loss_box_reg: 0.3793  loss_mask: 0.2768  loss_rpn_cls: 0.01613  loss_rpn_loc: 0.01722  time: 0.6904  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:51:58 d2.utils.events]: \u001b[0m eta: 0:41:55  iter: 11339  total_loss: 1.163  loss_cls: 0.239  loss_box_reg: 0.4869  loss_mask: 0.3456  loss_rpn_cls: 0.02093  loss_rpn_loc: 0.02719  time: 0.6900  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:52:11 d2.utils.events]: \u001b[0m eta: 0:41:33  iter: 11359  total_loss: 0.9884  loss_cls: 0.1666  loss_box_reg: 0.4323  loss_mask: 0.3652  loss_rpn_cls: 0.01682  loss_rpn_loc: 0.02413  time: 0.6898  data_time: 0.0070  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:52:25 d2.utils.events]: \u001b[0m eta: 0:41:22  iter: 11379  total_loss: 0.8114  loss_cls: 0.134  loss_box_reg: 0.4076  loss_mask: 0.2566  loss_rpn_cls: 0.01042  loss_rpn_loc: 0.014  time: 0.6899  data_time: 0.0065  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:52:39 d2.utils.events]: \u001b[0m eta: 0:41:15  iter: 11399  total_loss: 1.14  loss_cls: 0.1949  loss_box_reg: 0.5225  loss_mask: 0.3407  loss_rpn_cls: 0.01267  loss_rpn_loc: 0.01859  time: 0.6900  data_time: 0.0049  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:52:53 d2.utils.events]: \u001b[0m eta: 0:41:01  iter: 11419  total_loss: 1.08  loss_cls: 0.1866  loss_box_reg: 0.4226  loss_mask: 0.3617  loss_rpn_cls: 0.02503  loss_rpn_loc: 0.03382  time: 0.6902  data_time: 0.0050  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:53:07 d2.utils.events]: \u001b[0m eta: 0:40:51  iter: 11439  total_loss: 0.9402  loss_cls: 0.1851  loss_box_reg: 0.461  loss_mask: 0.3034  loss_rpn_cls: 0.02043  loss_rpn_loc: 0.02116  time: 0.6903  data_time: 0.0055  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:53:21 d2.utils.events]: \u001b[0m eta: 0:40:33  iter: 11459  total_loss: 0.9907  loss_cls: 0.1753  loss_box_reg: 0.439  loss_mask: 0.3265  loss_rpn_cls: 0.01229  loss_rpn_loc: 0.0204  time: 0.6901  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:53:34 d2.utils.events]: \u001b[0m eta: 0:40:14  iter: 11479  total_loss: 1.034  loss_cls: 0.2145  loss_box_reg: 0.4666  loss_mask: 0.3147  loss_rpn_cls: 0.01896  loss_rpn_loc: 0.02042  time: 0.6899  data_time: 0.0070  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:53:48 d2.utils.events]: \u001b[0m eta: 0:39:58  iter: 11499  total_loss: 1.066  loss_cls: 0.183  loss_box_reg: 0.4602  loss_mask: 0.34  loss_rpn_cls: 0.01609  loss_rpn_loc: 0.01646  time: 0.6899  data_time: 0.0058  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:54:01 d2.utils.events]: \u001b[0m eta: 0:39:37  iter: 11519  total_loss: 1.027  loss_cls: 0.2011  loss_box_reg: 0.4814  loss_mask: 0.3044  loss_rpn_cls: 0.01323  loss_rpn_loc: 0.01804  time: 0.6895  data_time: 0.0065  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:54:15 d2.utils.events]: \u001b[0m eta: 0:39:23  iter: 11539  total_loss: 1.075  loss_cls: 0.1897  loss_box_reg: 0.4631  loss_mask: 0.3419  loss_rpn_cls: 0.0177  loss_rpn_loc: 0.0177  time: 0.6893  data_time: 0.0053  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:54:28 d2.utils.events]: \u001b[0m eta: 0:39:09  iter: 11559  total_loss: 0.8822  loss_cls: 0.1408  loss_box_reg: 0.3931  loss_mask: 0.275  loss_rpn_cls: 0.01686  loss_rpn_loc: 0.02086  time: 0.6890  data_time: 0.0070  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:54:42 d2.utils.events]: \u001b[0m eta: 0:38:54  iter: 11579  total_loss: 1.053  loss_cls: 0.2159  loss_box_reg: 0.44  loss_mask: 0.3204  loss_rpn_cls: 0.01285  loss_rpn_loc: 0.02169  time: 0.6888  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:54:56 d2.utils.events]: \u001b[0m eta: 0:38:40  iter: 11599  total_loss: 0.8016  loss_cls: 0.1675  loss_box_reg: 0.3129  loss_mask: 0.3004  loss_rpn_cls: 0.01473  loss_rpn_loc: 0.01692  time: 0.6890  data_time: 0.0062  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:55:09 d2.utils.events]: \u001b[0m eta: 0:38:22  iter: 11619  total_loss: 0.8148  loss_cls: 0.1503  loss_box_reg: 0.356  loss_mask: 0.322  loss_rpn_cls: 0.01639  loss_rpn_loc: 0.01169  time: 0.6886  data_time: 0.0065  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:55:23 d2.utils.events]: \u001b[0m eta: 0:38:11  iter: 11639  total_loss: 1.001  loss_cls: 0.1599  loss_box_reg: 0.4453  loss_mask: 0.3467  loss_rpn_cls: 0.01948  loss_rpn_loc: 0.02277  time: 0.6887  data_time: 0.0051  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:55:37 d2.utils.events]: \u001b[0m eta: 0:37:56  iter: 11659  total_loss: 0.9812  loss_cls: 0.1692  loss_box_reg: 0.4396  loss_mask: 0.288  loss_rpn_cls: 0.01849  loss_rpn_loc: 0.02195  time: 0.6887  data_time: 0.0073  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:55:50 d2.utils.events]: \u001b[0m eta: 0:37:41  iter: 11679  total_loss: 1.023  loss_cls: 0.1854  loss_box_reg: 0.4493  loss_mask: 0.3391  loss_rpn_cls: 0.01777  loss_rpn_loc: 0.0183  time: 0.6884  data_time: 0.0043  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:56:04 d2.utils.events]: \u001b[0m eta: 0:37:28  iter: 11699  total_loss: 0.9284  loss_cls: 0.161  loss_box_reg: 0.4333  loss_mask: 0.3003  loss_rpn_cls: 0.01753  loss_rpn_loc: 0.01742  time: 0.6884  data_time: 0.0072  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:56:18 d2.utils.events]: \u001b[0m eta: 0:37:14  iter: 11719  total_loss: 0.985  loss_cls: 0.1552  loss_box_reg: 0.483  loss_mask: 0.3214  loss_rpn_cls: 0.01339  loss_rpn_loc: 0.01711  time: 0.6887  data_time: 0.0082  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:56:32 d2.utils.events]: \u001b[0m eta: 0:36:59  iter: 11739  total_loss: 0.8049  loss_cls: 0.121  loss_box_reg: 0.3801  loss_mask: 0.2971  loss_rpn_cls: 0.01327  loss_rpn_loc: 0.01374  time: 0.6886  data_time: 0.0067  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:56:45 d2.utils.events]: \u001b[0m eta: 0:36:48  iter: 11759  total_loss: 1.115  loss_cls: 0.1808  loss_box_reg: 0.5108  loss_mask: 0.3186  loss_rpn_cls: 0.01705  loss_rpn_loc: 0.02041  time: 0.6886  data_time: 0.0049  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:56:59 d2.utils.events]: \u001b[0m eta: 0:36:35  iter: 11779  total_loss: 1.029  loss_cls: 0.2354  loss_box_reg: 0.4662  loss_mask: 0.3263  loss_rpn_cls: 0.01848  loss_rpn_loc: 0.01877  time: 0.6884  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:57:13 d2.utils.events]: \u001b[0m eta: 0:36:28  iter: 11799  total_loss: 1.082  loss_cls: 0.1884  loss_box_reg: 0.4945  loss_mask: 0.3579  loss_rpn_cls: 0.0151  loss_rpn_loc: 0.02579  time: 0.6888  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:57:26 d2.utils.events]: \u001b[0m eta: 0:36:12  iter: 11819  total_loss: 1.143  loss_cls: 0.2274  loss_box_reg: 0.5353  loss_mask: 0.2952  loss_rpn_cls: 0.01944  loss_rpn_loc: 0.02201  time: 0.6884  data_time: 0.0062  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:57:40 d2.utils.events]: \u001b[0m eta: 0:35:58  iter: 11839  total_loss: 1.148  loss_cls: 0.1887  loss_box_reg: 0.4914  loss_mask: 0.361  loss_rpn_cls: 0.0298  loss_rpn_loc: 0.02486  time: 0.6883  data_time: 0.0057  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:57:54 d2.utils.events]: \u001b[0m eta: 0:35:44  iter: 11859  total_loss: 1.137  loss_cls: 0.2003  loss_box_reg: 0.5481  loss_mask: 0.3439  loss_rpn_cls: 0.01601  loss_rpn_loc: 0.01872  time: 0.6884  data_time: 0.0065  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:58:08 d2.utils.events]: \u001b[0m eta: 0:35:35  iter: 11879  total_loss: 0.8321  loss_cls: 0.1658  loss_box_reg: 0.3632  loss_mask: 0.2972  loss_rpn_cls: 0.01467  loss_rpn_loc: 0.01563  time: 0.6885  data_time: 0.0052  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:58:22 d2.utils.events]: \u001b[0m eta: 0:35:23  iter: 11899  total_loss: 1.024  loss_cls: 0.1874  loss_box_reg: 0.4594  loss_mask: 0.3012  loss_rpn_cls: 0.01325  loss_rpn_loc: 0.0146  time: 0.6886  data_time: 0.0055  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:58:35 d2.utils.events]: \u001b[0m eta: 0:35:02  iter: 11919  total_loss: 1.289  loss_cls: 0.2626  loss_box_reg: 0.507  loss_mask: 0.3672  loss_rpn_cls: 0.02052  loss_rpn_loc: 0.02853  time: 0.6882  data_time: 0.0075  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:58:48 d2.utils.events]: \u001b[0m eta: 0:34:48  iter: 11939  total_loss: 0.9695  loss_cls: 0.2041  loss_box_reg: 0.4526  loss_mask: 0.2695  loss_rpn_cls: 0.01492  loss_rpn_loc: 0.01882  time: 0.6881  data_time: 0.0076  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:59:02 d2.utils.events]: \u001b[0m eta: 0:34:36  iter: 11959  total_loss: 1.003  loss_cls: 0.1585  loss_box_reg: 0.4393  loss_mask: 0.3517  loss_rpn_cls: 0.01744  loss_rpn_loc: 0.01653  time: 0.6881  data_time: 0.0052  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:59:16 d2.utils.events]: \u001b[0m eta: 0:34:23  iter: 11979  total_loss: 1.081  loss_cls: 0.1606  loss_box_reg: 0.4464  loss_mask: 0.3079  loss_rpn_cls: 0.01711  loss_rpn_loc: 0.01698  time: 0.6881  data_time: 0.0053  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:59:30 d2.utils.events]: \u001b[0m eta: 0:34:11  iter: 11999  total_loss: 1.122  loss_cls: 0.2222  loss_box_reg: 0.4629  loss_mask: 0.3497  loss_rpn_cls: 0.02151  loss_rpn_loc: 0.02456  time: 0.6880  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:59:43 d2.utils.events]: \u001b[0m eta: 0:33:56  iter: 12019  total_loss: 1.068  loss_cls: 0.1695  loss_box_reg: 0.4936  loss_mask: 0.3276  loss_rpn_cls: 0.01803  loss_rpn_loc: 0.01546  time: 0.6876  data_time: 0.0064  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 14:59:57 d2.utils.events]: \u001b[0m eta: 0:33:44  iter: 12039  total_loss: 1.01  loss_cls: 0.194  loss_box_reg: 0.4511  loss_mask: 0.3227  loss_rpn_cls: 0.01608  loss_rpn_loc: 0.02002  time: 0.6878  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:00:10 d2.utils.events]: \u001b[0m eta: 0:33:29  iter: 12059  total_loss: 1.07  loss_cls: 0.1725  loss_box_reg: 0.4247  loss_mask: 0.3081  loss_rpn_cls: 0.0146  loss_rpn_loc: 0.01973  time: 0.6877  data_time: 0.0064  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:00:24 d2.utils.events]: \u001b[0m eta: 0:33:15  iter: 12079  total_loss: 0.9017  loss_cls: 0.1898  loss_box_reg: 0.3841  loss_mask: 0.2668  loss_rpn_cls: 0.0125  loss_rpn_loc: 0.02007  time: 0.6877  data_time: 0.0079  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:00:38 d2.utils.events]: \u001b[0m eta: 0:33:01  iter: 12099  total_loss: 1.039  loss_cls: 0.1642  loss_box_reg: 0.4644  loss_mask: 0.3113  loss_rpn_cls: 0.01363  loss_rpn_loc: 0.01882  time: 0.6877  data_time: 0.0058  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:00:51 d2.utils.events]: \u001b[0m eta: 0:32:46  iter: 12119  total_loss: 0.8337  loss_cls: 0.1293  loss_box_reg: 0.3781  loss_mask: 0.3082  loss_rpn_cls: 0.01451  loss_rpn_loc: 0.01455  time: 0.6873  data_time: 0.0069  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:01:05 d2.utils.events]: \u001b[0m eta: 0:32:35  iter: 12139  total_loss: 0.8605  loss_cls: 0.1602  loss_box_reg: 0.4063  loss_mask: 0.2746  loss_rpn_cls: 0.01115  loss_rpn_loc: 0.01067  time: 0.6874  data_time: 0.0062  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:01:19 d2.utils.events]: \u001b[0m eta: 0:32:23  iter: 12159  total_loss: 0.877  loss_cls: 0.1891  loss_box_reg: 0.4542  loss_mask: 0.2741  loss_rpn_cls: 0.01511  loss_rpn_loc: 0.0218  time: 0.6875  data_time: 0.0068  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:01:33 d2.utils.events]: \u001b[0m eta: 0:32:11  iter: 12179  total_loss: 0.964  loss_cls: 0.1723  loss_box_reg: 0.4065  loss_mask: 0.3255  loss_rpn_cls: 0.01556  loss_rpn_loc: 0.01509  time: 0.6878  data_time: 0.0069  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:01:47 d2.utils.events]: \u001b[0m eta: 0:31:55  iter: 12199  total_loss: 1.029  loss_cls: 0.1415  loss_box_reg: 0.4818  loss_mask: 0.3445  loss_rpn_cls: 0.01045  loss_rpn_loc: 0.01267  time: 0.6878  data_time: 0.0069  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:02:01 d2.utils.events]: \u001b[0m eta: 0:31:42  iter: 12219  total_loss: 1.075  loss_cls: 0.2031  loss_box_reg: 0.4987  loss_mask: 0.365  loss_rpn_cls: 0.02184  loss_rpn_loc: 0.02459  time: 0.6878  data_time: 0.0062  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:02:14 d2.utils.events]: \u001b[0m eta: 0:31:28  iter: 12239  total_loss: 0.8905  loss_cls: 0.1633  loss_box_reg: 0.415  loss_mask: 0.2957  loss_rpn_cls: 0.01107  loss_rpn_loc: 0.01362  time: 0.6876  data_time: 0.0054  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:02:28 d2.utils.events]: \u001b[0m eta: 0:31:14  iter: 12259  total_loss: 0.865  loss_cls: 0.1381  loss_box_reg: 0.3966  loss_mask: 0.3162  loss_rpn_cls: 0.01513  loss_rpn_loc: 0.02369  time: 0.6877  data_time: 0.0057  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:02:41 d2.utils.events]: \u001b[0m eta: 0:30:56  iter: 12279  total_loss: 1.047  loss_cls: 0.1975  loss_box_reg: 0.4338  loss_mask: 0.3381  loss_rpn_cls: 0.01689  loss_rpn_loc: 0.02071  time: 0.6875  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:02:55 d2.utils.events]: \u001b[0m eta: 0:30:44  iter: 12299  total_loss: 1.062  loss_cls: 0.2171  loss_box_reg: 0.4944  loss_mask: 0.3371  loss_rpn_cls: 0.01627  loss_rpn_loc: 0.02157  time: 0.6874  data_time: 0.0075  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:03:08 d2.utils.events]: \u001b[0m eta: 0:30:29  iter: 12319  total_loss: 1.01  loss_cls: 0.1515  loss_box_reg: 0.4414  loss_mask: 0.3251  loss_rpn_cls: 0.01875  loss_rpn_loc: 0.01764  time: 0.6873  data_time: 0.0060  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:03:21 d2.utils.events]: \u001b[0m eta: 0:30:15  iter: 12339  total_loss: 1.053  loss_cls: 0.1768  loss_box_reg: 0.4807  loss_mask: 0.3342  loss_rpn_cls: 0.01985  loss_rpn_loc: 0.01834  time: 0.6870  data_time: 0.0064  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:03:36 d2.utils.events]: \u001b[0m eta: 0:30:04  iter: 12359  total_loss: 1.126  loss_cls: 0.2246  loss_box_reg: 0.4522  loss_mask: 0.3254  loss_rpn_cls: 0.01997  loss_rpn_loc: 0.03004  time: 0.6872  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:03:49 d2.utils.events]: \u001b[0m eta: 0:29:48  iter: 12379  total_loss: 0.904  loss_cls: 0.1601  loss_box_reg: 0.4201  loss_mask: 0.2701  loss_rpn_cls: 0.01336  loss_rpn_loc: 0.01368  time: 0.6871  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:04:03 d2.utils.events]: \u001b[0m eta: 0:29:33  iter: 12399  total_loss: 0.8051  loss_cls: 0.1601  loss_box_reg: 0.3359  loss_mask: 0.3021  loss_rpn_cls: 0.01032  loss_rpn_loc: 0.01923  time: 0.6871  data_time: 0.0064  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:04:17 d2.utils.events]: \u001b[0m eta: 0:29:19  iter: 12419  total_loss: 0.8843  loss_cls: 0.1491  loss_box_reg: 0.4037  loss_mask: 0.3146  loss_rpn_cls: 0.01456  loss_rpn_loc: 0.02151  time: 0.6872  data_time: 0.0061  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:04:31 d2.utils.events]: \u001b[0m eta: 0:29:06  iter: 12439  total_loss: 0.9171  loss_cls: 0.1432  loss_box_reg: 0.4533  loss_mask: 0.2877  loss_rpn_cls: 0.01614  loss_rpn_loc: 0.01589  time: 0.6873  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:04:44 d2.utils.events]: \u001b[0m eta: 0:28:52  iter: 12459  total_loss: 1.071  loss_cls: 0.19  loss_box_reg: 0.5079  loss_mask: 0.3213  loss_rpn_cls: 0.01592  loss_rpn_loc: 0.02106  time: 0.6871  data_time: 0.0049  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:04:58 d2.utils.events]: \u001b[0m eta: 0:28:39  iter: 12479  total_loss: 1.035  loss_cls: 0.1869  loss_box_reg: 0.4679  loss_mask: 0.334  loss_rpn_cls: 0.019  loss_rpn_loc: 0.02024  time: 0.6870  data_time: 0.0067  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:05:12 d2.utils.events]: \u001b[0m eta: 0:28:27  iter: 12499  total_loss: 0.9824  loss_cls: 0.1906  loss_box_reg: 0.4112  loss_mask: 0.2918  loss_rpn_cls: 0.01277  loss_rpn_loc: 0.01763  time: 0.6871  data_time: 0.0052  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:05:25 d2.utils.events]: \u001b[0m eta: 0:28:15  iter: 12519  total_loss: 0.9553  loss_cls: 0.2056  loss_box_reg: 0.4579  loss_mask: 0.2993  loss_rpn_cls: 0.02012  loss_rpn_loc: 0.02294  time: 0.6869  data_time: 0.0066  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:05:39 d2.utils.events]: \u001b[0m eta: 0:28:04  iter: 12539  total_loss: 0.8287  loss_cls: 0.1624  loss_box_reg: 0.378  loss_mask: 0.2801  loss_rpn_cls: 0.01514  loss_rpn_loc: 0.01586  time: 0.6870  data_time: 0.0064  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:05:53 d2.utils.events]: \u001b[0m eta: 0:27:57  iter: 12559  total_loss: 1.096  loss_cls: 0.2129  loss_box_reg: 0.4738  loss_mask: 0.3535  loss_rpn_cls: 0.01749  loss_rpn_loc: 0.01898  time: 0.6873  data_time: 0.0044  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:06:07 d2.utils.events]: \u001b[0m eta: 0:27:44  iter: 12579  total_loss: 1.116  loss_cls: 0.2236  loss_box_reg: 0.4981  loss_mask: 0.3164  loss_rpn_cls: 0.01909  loss_rpn_loc: 0.01639  time: 0.6873  data_time: 0.0051  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:06:21 d2.utils.events]: \u001b[0m eta: 0:27:29  iter: 12599  total_loss: 1.059  loss_cls: 0.1734  loss_box_reg: 0.4562  loss_mask: 0.3023  loss_rpn_cls: 0.01419  loss_rpn_loc: 0.02713  time: 0.6874  data_time: 0.0066  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:06:35 d2.utils.events]: \u001b[0m eta: 0:27:21  iter: 12619  total_loss: 1.04  loss_cls: 0.2087  loss_box_reg: 0.4848  loss_mask: 0.3237  loss_rpn_cls: 0.0167  loss_rpn_loc: 0.02658  time: 0.6874  data_time: 0.0053  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:06:49 d2.utils.events]: \u001b[0m eta: 0:27:06  iter: 12639  total_loss: 0.8866  loss_cls: 0.1736  loss_box_reg: 0.4221  loss_mask: 0.3137  loss_rpn_cls: 0.01385  loss_rpn_loc: 0.01787  time: 0.6874  data_time: 0.0083  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:07:02 d2.utils.events]: \u001b[0m eta: 0:26:50  iter: 12659  total_loss: 0.9299  loss_cls: 0.1476  loss_box_reg: 0.3874  loss_mask: 0.3288  loss_rpn_cls: 0.01347  loss_rpn_loc: 0.01727  time: 0.6873  data_time: 0.0072  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:07:16 d2.utils.events]: \u001b[0m eta: 0:26:39  iter: 12679  total_loss: 1.075  loss_cls: 0.2167  loss_box_reg: 0.4669  loss_mask: 0.3321  loss_rpn_cls: 0.01785  loss_rpn_loc: 0.02706  time: 0.6873  data_time: 0.0076  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:07:30 d2.utils.events]: \u001b[0m eta: 0:26:26  iter: 12699  total_loss: 0.7648  loss_cls: 0.1269  loss_box_reg: 0.3823  loss_mask: 0.3001  loss_rpn_cls: 0.01851  loss_rpn_loc: 0.02022  time: 0.6876  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:07:44 d2.utils.events]: \u001b[0m eta: 0:26:07  iter: 12719  total_loss: 0.8357  loss_cls: 0.1498  loss_box_reg: 0.416  loss_mask: 0.2847  loss_rpn_cls: 0.0164  loss_rpn_loc: 0.01398  time: 0.6873  data_time: 0.0061  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:07:57 d2.utils.events]: \u001b[0m eta: 0:25:53  iter: 12739  total_loss: 1.005  loss_cls: 0.1712  loss_box_reg: 0.443  loss_mask: 0.3439  loss_rpn_cls: 0.01457  loss_rpn_loc: 0.03113  time: 0.6873  data_time: 0.0078  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:08:11 d2.utils.events]: \u001b[0m eta: 0:25:39  iter: 12759  total_loss: 1.066  loss_cls: 0.2057  loss_box_reg: 0.4964  loss_mask: 0.3408  loss_rpn_cls: 0.01753  loss_rpn_loc: 0.02188  time: 0.6874  data_time: 0.0065  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:08:25 d2.utils.events]: \u001b[0m eta: 0:25:26  iter: 12779  total_loss: 0.9367  loss_cls: 0.1529  loss_box_reg: 0.363  loss_mask: 0.3282  loss_rpn_cls: 0.01404  loss_rpn_loc: 0.0198  time: 0.6873  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:08:38 d2.utils.events]: \u001b[0m eta: 0:25:07  iter: 12799  total_loss: 0.8559  loss_cls: 0.137  loss_box_reg: 0.4114  loss_mask: 0.2942  loss_rpn_cls: 0.01607  loss_rpn_loc: 0.01596  time: 0.6872  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:08:52 d2.utils.events]: \u001b[0m eta: 0:24:54  iter: 12819  total_loss: 0.9586  loss_cls: 0.164  loss_box_reg: 0.4637  loss_mask: 0.3352  loss_rpn_cls: 0.01446  loss_rpn_loc: 0.02166  time: 0.6872  data_time: 0.0068  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:09:05 d2.utils.events]: \u001b[0m eta: 0:24:39  iter: 12839  total_loss: 0.9459  loss_cls: 0.1415  loss_box_reg: 0.4209  loss_mask: 0.3015  loss_rpn_cls: 0.01896  loss_rpn_loc: 0.01605  time: 0.6870  data_time: 0.0067  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:09:19 d2.utils.events]: \u001b[0m eta: 0:24:23  iter: 12859  total_loss: 0.9624  loss_cls: 0.1615  loss_box_reg: 0.4423  loss_mask: 0.3387  loss_rpn_cls: 0.01409  loss_rpn_loc: 0.01419  time: 0.6869  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:09:33 d2.utils.events]: \u001b[0m eta: 0:24:08  iter: 12879  total_loss: 1.009  loss_cls: 0.1885  loss_box_reg: 0.4535  loss_mask: 0.2973  loss_rpn_cls: 0.01677  loss_rpn_loc: 0.01901  time: 0.6869  data_time: 0.0060  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:09:46 d2.utils.events]: \u001b[0m eta: 0:23:53  iter: 12899  total_loss: 0.9554  loss_cls: 0.1494  loss_box_reg: 0.4458  loss_mask: 0.3094  loss_rpn_cls: 0.01369  loss_rpn_loc: 0.01714  time: 0.6869  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:10:00 d2.utils.events]: \u001b[0m eta: 0:23:42  iter: 12919  total_loss: 0.8316  loss_cls: 0.1627  loss_box_reg: 0.4287  loss_mask: 0.2862  loss_rpn_cls: 0.01386  loss_rpn_loc: 0.0264  time: 0.6869  data_time: 0.0070  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:10:14 d2.utils.events]: \u001b[0m eta: 0:23:29  iter: 12939  total_loss: 0.9202  loss_cls: 0.1254  loss_box_reg: 0.4007  loss_mask: 0.3199  loss_rpn_cls: 0.01396  loss_rpn_loc: 0.01592  time: 0.6869  data_time: 0.0061  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:10:28 d2.utils.events]: \u001b[0m eta: 0:23:18  iter: 12959  total_loss: 0.9859  loss_cls: 0.1501  loss_box_reg: 0.4561  loss_mask: 0.3152  loss_rpn_cls: 0.01326  loss_rpn_loc: 0.01872  time: 0.6870  data_time: 0.0051  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:10:42 d2.utils.events]: \u001b[0m eta: 0:23:05  iter: 12979  total_loss: 0.862  loss_cls: 0.1396  loss_box_reg: 0.3819  loss_mask: 0.3146  loss_rpn_cls: 0.01942  loss_rpn_loc: 0.0243  time: 0.6870  data_time: 0.0061  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:10:56 d2.utils.events]: \u001b[0m eta: 0:22:51  iter: 12999  total_loss: 1.129  loss_cls: 0.218  loss_box_reg: 0.5546  loss_mask: 0.3239  loss_rpn_cls: 0.01655  loss_rpn_loc: 0.02596  time: 0.6871  data_time: 0.0074  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:11:09 d2.utils.events]: \u001b[0m eta: 0:22:39  iter: 13019  total_loss: 1.033  loss_cls: 0.2218  loss_box_reg: 0.4737  loss_mask: 0.3355  loss_rpn_cls: 0.01562  loss_rpn_loc: 0.01629  time: 0.6871  data_time: 0.0068  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:11:24 d2.utils.events]: \u001b[0m eta: 0:22:25  iter: 13039  total_loss: 0.8933  loss_cls: 0.1452  loss_box_reg: 0.3852  loss_mask: 0.2964  loss_rpn_cls: 0.01942  loss_rpn_loc: 0.01481  time: 0.6873  data_time: 0.0062  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:11:37 d2.utils.events]: \u001b[0m eta: 0:22:12  iter: 13059  total_loss: 0.9546  loss_cls: 0.1603  loss_box_reg: 0.4047  loss_mask: 0.3131  loss_rpn_cls: 0.008939  loss_rpn_loc: 0.01769  time: 0.6873  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:11:51 d2.utils.events]: \u001b[0m eta: 0:21:58  iter: 13079  total_loss: 0.9285  loss_cls: 0.1425  loss_box_reg: 0.3514  loss_mask: 0.3066  loss_rpn_cls: 0.01559  loss_rpn_loc: 0.02171  time: 0.6873  data_time: 0.0064  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:12:05 d2.utils.events]: \u001b[0m eta: 0:21:44  iter: 13099  total_loss: 1.101  loss_cls: 0.2171  loss_box_reg: 0.4508  loss_mask: 0.3333  loss_rpn_cls: 0.0218  loss_rpn_loc: 0.02797  time: 0.6873  data_time: 0.0054  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:12:19 d2.utils.events]: \u001b[0m eta: 0:21:35  iter: 13119  total_loss: 0.9663  loss_cls: 0.1458  loss_box_reg: 0.4318  loss_mask: 0.3176  loss_rpn_cls: 0.02307  loss_rpn_loc: 0.03126  time: 0.6873  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:12:33 d2.utils.events]: \u001b[0m eta: 0:21:22  iter: 13139  total_loss: 0.8382  loss_cls: 0.1374  loss_box_reg: 0.3729  loss_mask: 0.2561  loss_rpn_cls: 0.01151  loss_rpn_loc: 0.01433  time: 0.6874  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:12:47 d2.utils.events]: \u001b[0m eta: 0:21:08  iter: 13159  total_loss: 0.9499  loss_cls: 0.1502  loss_box_reg: 0.4135  loss_mask: 0.301  loss_rpn_cls: 0.02115  loss_rpn_loc: 0.01901  time: 0.6875  data_time: 0.0068  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:13:01 d2.utils.events]: \u001b[0m eta: 0:20:53  iter: 13179  total_loss: 0.843  loss_cls: 0.1457  loss_box_reg: 0.3608  loss_mask: 0.2888  loss_rpn_cls: 0.01152  loss_rpn_loc: 0.0178  time: 0.6875  data_time: 0.0066  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:13:14 d2.utils.events]: \u001b[0m eta: 0:20:39  iter: 13199  total_loss: 0.961  loss_cls: 0.1751  loss_box_reg: 0.426  loss_mask: 0.3361  loss_rpn_cls: 0.01392  loss_rpn_loc: 0.01624  time: 0.6875  data_time: 0.0075  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:13:28 d2.utils.events]: \u001b[0m eta: 0:20:26  iter: 13219  total_loss: 0.8591  loss_cls: 0.1771  loss_box_reg: 0.4082  loss_mask: 0.269  loss_rpn_cls: 0.01143  loss_rpn_loc: 0.01202  time: 0.6874  data_time: 0.0048  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:13:42 d2.utils.events]: \u001b[0m eta: 0:20:12  iter: 13239  total_loss: 0.8748  loss_cls: 0.1765  loss_box_reg: 0.4182  loss_mask: 0.2967  loss_rpn_cls: 0.01475  loss_rpn_loc: 0.01611  time: 0.6874  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:13:56 d2.utils.events]: \u001b[0m eta: 0:19:58  iter: 13259  total_loss: 0.9478  loss_cls: 0.1675  loss_box_reg: 0.4062  loss_mask: 0.2943  loss_rpn_cls: 0.01203  loss_rpn_loc: 0.02204  time: 0.6875  data_time: 0.0070  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:14:09 d2.utils.events]: \u001b[0m eta: 0:19:44  iter: 13279  total_loss: 0.9696  loss_cls: 0.2002  loss_box_reg: 0.4131  loss_mask: 0.2617  loss_rpn_cls: 0.02431  loss_rpn_loc: 0.02629  time: 0.6874  data_time: 0.0060  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:14:23 d2.utils.events]: \u001b[0m eta: 0:19:31  iter: 13299  total_loss: 0.9797  loss_cls: 0.2123  loss_box_reg: 0.3997  loss_mask: 0.3213  loss_rpn_cls: 0.0108  loss_rpn_loc: 0.01479  time: 0.6875  data_time: 0.0080  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:14:36 d2.utils.events]: \u001b[0m eta: 0:19:17  iter: 13319  total_loss: 1.057  loss_cls: 0.1895  loss_box_reg: 0.4712  loss_mask: 0.3104  loss_rpn_cls: 0.01758  loss_rpn_loc: 0.01393  time: 0.6873  data_time: 0.0067  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:14:51 d2.utils.events]: \u001b[0m eta: 0:19:04  iter: 13339  total_loss: 1.121  loss_cls: 0.1959  loss_box_reg: 0.4521  loss_mask: 0.2811  loss_rpn_cls: 0.01457  loss_rpn_loc: 0.01885  time: 0.6875  data_time: 0.0054  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:15:05 d2.utils.events]: \u001b[0m eta: 0:18:51  iter: 13359  total_loss: 0.9771  loss_cls: 0.1863  loss_box_reg: 0.4622  loss_mask: 0.3238  loss_rpn_cls: 0.0183  loss_rpn_loc: 0.02065  time: 0.6875  data_time: 0.0074  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:15:19 d2.utils.events]: \u001b[0m eta: 0:18:37  iter: 13379  total_loss: 0.9804  loss_cls: 0.1797  loss_box_reg: 0.4491  loss_mask: 0.2817  loss_rpn_cls: 0.01178  loss_rpn_loc: 0.01523  time: 0.6876  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:15:32 d2.utils.events]: \u001b[0m eta: 0:18:23  iter: 13399  total_loss: 0.7564  loss_cls: 0.1074  loss_box_reg: 0.3038  loss_mask: 0.2378  loss_rpn_cls: 0.01301  loss_rpn_loc: 0.01349  time: 0.6875  data_time: 0.0048  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:15:46 d2.utils.events]: \u001b[0m eta: 0:18:08  iter: 13419  total_loss: 0.9266  loss_cls: 0.1767  loss_box_reg: 0.4344  loss_mask: 0.3066  loss_rpn_cls: 0.01321  loss_rpn_loc: 0.01681  time: 0.6875  data_time: 0.0064  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:15:59 d2.utils.events]: \u001b[0m eta: 0:17:54  iter: 13439  total_loss: 0.7875  loss_cls: 0.1218  loss_box_reg: 0.3285  loss_mask: 0.274  loss_rpn_cls: 0.009316  loss_rpn_loc: 0.01107  time: 0.6875  data_time: 0.0064  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:16:13 d2.utils.events]: \u001b[0m eta: 0:17:41  iter: 13459  total_loss: 1.004  loss_cls: 0.1748  loss_box_reg: 0.4488  loss_mask: 0.3183  loss_rpn_cls: 0.01497  loss_rpn_loc: 0.02053  time: 0.6873  data_time: 0.0058  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:16:26 d2.utils.events]: \u001b[0m eta: 0:17:27  iter: 13479  total_loss: 0.9445  loss_cls: 0.1576  loss_box_reg: 0.4337  loss_mask: 0.3032  loss_rpn_cls: 0.01045  loss_rpn_loc: 0.01586  time: 0.6873  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:16:40 d2.utils.events]: \u001b[0m eta: 0:17:14  iter: 13499  total_loss: 0.9049  loss_cls: 0.1429  loss_box_reg: 0.4242  loss_mask: 0.2673  loss_rpn_cls: 0.01718  loss_rpn_loc: 0.01887  time: 0.6874  data_time: 0.0058  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:16:54 d2.utils.events]: \u001b[0m eta: 0:17:00  iter: 13519  total_loss: 0.8646  loss_cls: 0.1484  loss_box_reg: 0.3895  loss_mask: 0.2476  loss_rpn_cls: 0.01164  loss_rpn_loc: 0.01561  time: 0.6874  data_time: 0.0058  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:17:07 d2.utils.events]: \u001b[0m eta: 0:16:46  iter: 13539  total_loss: 0.8531  loss_cls: 0.1251  loss_box_reg: 0.3806  loss_mask: 0.3086  loss_rpn_cls: 0.01327  loss_rpn_loc: 0.01862  time: 0.6872  data_time: 0.0073  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:17:21 d2.utils.events]: \u001b[0m eta: 0:16:31  iter: 13559  total_loss: 0.9737  loss_cls: 0.1725  loss_box_reg: 0.4397  loss_mask: 0.3515  loss_rpn_cls: 0.01999  loss_rpn_loc: 0.02586  time: 0.6873  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:17:35 d2.utils.events]: \u001b[0m eta: 0:16:18  iter: 13579  total_loss: 1.03  loss_cls: 0.1937  loss_box_reg: 0.4609  loss_mask: 0.2905  loss_rpn_cls: 0.01931  loss_rpn_loc: 0.02424  time: 0.6874  data_time: 0.0053  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:17:50 d2.utils.events]: \u001b[0m eta: 0:16:05  iter: 13599  total_loss: 1.023  loss_cls: 0.1574  loss_box_reg: 0.4387  loss_mask: 0.3167  loss_rpn_cls: 0.01383  loss_rpn_loc: 0.0172  time: 0.6875  data_time: 0.0051  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:18:03 d2.utils.events]: \u001b[0m eta: 0:15:49  iter: 13619  total_loss: 0.9952  loss_cls: 0.2044  loss_box_reg: 0.4659  loss_mask: 0.3061  loss_rpn_cls: 0.01538  loss_rpn_loc: 0.01707  time: 0.6873  data_time: 0.0055  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:18:17 d2.utils.events]: \u001b[0m eta: 0:15:35  iter: 13639  total_loss: 1.019  loss_cls: 0.2078  loss_box_reg: 0.4189  loss_mask: 0.3047  loss_rpn_cls: 0.01898  loss_rpn_loc: 0.0242  time: 0.6874  data_time: 0.0067  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:18:31 d2.utils.events]: \u001b[0m eta: 0:15:22  iter: 13659  total_loss: 1.02  loss_cls: 0.1801  loss_box_reg: 0.4374  loss_mask: 0.3035  loss_rpn_cls: 0.01553  loss_rpn_loc: 0.01871  time: 0.6874  data_time: 0.0065  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:18:45 d2.utils.events]: \u001b[0m eta: 0:15:09  iter: 13679  total_loss: 0.675  loss_cls: 0.09792  loss_box_reg: 0.3281  loss_mask: 0.2382  loss_rpn_cls: 0.01048  loss_rpn_loc: 0.01391  time: 0.6875  data_time: 0.0062  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:18:59 d2.utils.events]: \u001b[0m eta: 0:14:55  iter: 13699  total_loss: 0.9956  loss_cls: 0.181  loss_box_reg: 0.463  loss_mask: 0.2976  loss_rpn_cls: 0.01367  loss_rpn_loc: 0.02204  time: 0.6875  data_time: 0.0061  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:19:13 d2.utils.events]: \u001b[0m eta: 0:14:42  iter: 13719  total_loss: 1.044  loss_cls: 0.1871  loss_box_reg: 0.443  loss_mask: 0.335  loss_rpn_cls: 0.01687  loss_rpn_loc: 0.02728  time: 0.6876  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:19:26 d2.utils.events]: \u001b[0m eta: 0:14:29  iter: 13739  total_loss: 0.925  loss_cls: 0.1373  loss_box_reg: 0.3785  loss_mask: 0.301  loss_rpn_cls: 0.01326  loss_rpn_loc: 0.01722  time: 0.6876  data_time: 0.0051  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:19:41 d2.utils.events]: \u001b[0m eta: 0:14:16  iter: 13759  total_loss: 0.8116  loss_cls: 0.1184  loss_box_reg: 0.3805  loss_mask: 0.2696  loss_rpn_cls: 0.01202  loss_rpn_loc: 0.01686  time: 0.6878  data_time: 0.0061  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:19:54 d2.utils.events]: \u001b[0m eta: 0:14:02  iter: 13779  total_loss: 1.025  loss_cls: 0.1833  loss_box_reg: 0.4694  loss_mask: 0.335  loss_rpn_cls: 0.01557  loss_rpn_loc: 0.01592  time: 0.6877  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:20:08 d2.utils.events]: \u001b[0m eta: 0:13:50  iter: 13799  total_loss: 1.002  loss_cls: 0.1748  loss_box_reg: 0.4324  loss_mask: 0.3179  loss_rpn_cls: 0.02047  loss_rpn_loc: 0.03361  time: 0.6878  data_time: 0.0057  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:20:22 d2.utils.events]: \u001b[0m eta: 0:13:37  iter: 13819  total_loss: 1.085  loss_cls: 0.1931  loss_box_reg: 0.4779  loss_mask: 0.3189  loss_rpn_cls: 0.01549  loss_rpn_loc: 0.01895  time: 0.6878  data_time: 0.0064  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:20:36 d2.utils.events]: \u001b[0m eta: 0:13:24  iter: 13839  total_loss: 0.8587  loss_cls: 0.1376  loss_box_reg: 0.3996  loss_mask: 0.272  loss_rpn_cls: 0.01858  loss_rpn_loc: 0.01352  time: 0.6878  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:20:50 d2.utils.events]: \u001b[0m eta: 0:13:10  iter: 13859  total_loss: 0.9683  loss_cls: 0.2068  loss_box_reg: 0.4527  loss_mask: 0.3439  loss_rpn_cls: 0.01277  loss_rpn_loc: 0.02247  time: 0.6878  data_time: 0.0055  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:21:03 d2.utils.events]: \u001b[0m eta: 0:12:55  iter: 13879  total_loss: 0.8564  loss_cls: 0.1463  loss_box_reg: 0.4232  loss_mask: 0.3042  loss_rpn_cls: 0.01129  loss_rpn_loc: 0.01466  time: 0.6876  data_time: 0.0066  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:21:16 d2.utils.events]: \u001b[0m eta: 0:12:41  iter: 13899  total_loss: 1.039  loss_cls: 0.206  loss_box_reg: 0.4812  loss_mask: 0.3329  loss_rpn_cls: 0.01909  loss_rpn_loc: 0.02462  time: 0.6875  data_time: 0.0047  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:21:29 d2.utils.events]: \u001b[0m eta: 0:12:27  iter: 13919  total_loss: 1.014  loss_cls: 0.1699  loss_box_reg: 0.4762  loss_mask: 0.2993  loss_rpn_cls: 0.01955  loss_rpn_loc: 0.01648  time: 0.6874  data_time: 0.0057  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:21:43 d2.utils.events]: \u001b[0m eta: 0:12:12  iter: 13939  total_loss: 0.9426  loss_cls: 0.1901  loss_box_reg: 0.4447  loss_mask: 0.2789  loss_rpn_cls: 0.01711  loss_rpn_loc: 0.01851  time: 0.6873  data_time: 0.0051  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:21:56 d2.utils.events]: \u001b[0m eta: 0:11:57  iter: 13959  total_loss: 1.029  loss_cls: 0.1564  loss_box_reg: 0.4549  loss_mask: 0.3268  loss_rpn_cls: 0.01708  loss_rpn_loc: 0.0249  time: 0.6871  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:22:10 d2.utils.events]: \u001b[0m eta: 0:11:43  iter: 13979  total_loss: 0.9492  loss_cls: 0.1575  loss_box_reg: 0.4417  loss_mask: 0.2962  loss_rpn_cls: 0.01445  loss_rpn_loc: 0.01391  time: 0.6872  data_time: 0.0060  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:22:23 d2.utils.events]: \u001b[0m eta: 0:11:29  iter: 13999  total_loss: 0.995  loss_cls: 0.1813  loss_box_reg: 0.4305  loss_mask: 0.3278  loss_rpn_cls: 0.0169  loss_rpn_loc: 0.02368  time: 0.6871  data_time: 0.0060  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:22:37 d2.utils.events]: \u001b[0m eta: 0:11:15  iter: 14019  total_loss: 0.9408  loss_cls: 0.1372  loss_box_reg: 0.4126  loss_mask: 0.3061  loss_rpn_cls: 0.01429  loss_rpn_loc: 0.0179  time: 0.6871  data_time: 0.0055  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:22:51 d2.utils.events]: \u001b[0m eta: 0:11:01  iter: 14039  total_loss: 0.9494  loss_cls: 0.176  loss_box_reg: 0.4067  loss_mask: 0.306  loss_rpn_cls: 0.01468  loss_rpn_loc: 0.02199  time: 0.6871  data_time: 0.0060  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:23:04 d2.utils.events]: \u001b[0m eta: 0:10:46  iter: 14059  total_loss: 0.9306  loss_cls: 0.1745  loss_box_reg: 0.4581  loss_mask: 0.2936  loss_rpn_cls: 0.0135  loss_rpn_loc: 0.01718  time: 0.6871  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:23:18 d2.utils.events]: \u001b[0m eta: 0:10:32  iter: 14079  total_loss: 0.9393  loss_cls: 0.1582  loss_box_reg: 0.4017  loss_mask: 0.3282  loss_rpn_cls: 0.01743  loss_rpn_loc: 0.02113  time: 0.6869  data_time: 0.0059  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:23:31 d2.utils.events]: \u001b[0m eta: 0:10:18  iter: 14099  total_loss: 0.9149  loss_cls: 0.1697  loss_box_reg: 0.3763  loss_mask: 0.2585  loss_rpn_cls: 0.01053  loss_rpn_loc: 0.02144  time: 0.6868  data_time: 0.0055  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:23:44 d2.utils.events]: \u001b[0m eta: 0:10:04  iter: 14119  total_loss: 0.9311  loss_cls: 0.1696  loss_box_reg: 0.4617  loss_mask: 0.316  loss_rpn_cls: 0.009761  loss_rpn_loc: 0.01718  time: 0.6868  data_time: 0.0053  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:23:58 d2.utils.events]: \u001b[0m eta: 0:09:50  iter: 14139  total_loss: 0.9658  loss_cls: 0.1689  loss_box_reg: 0.4019  loss_mask: 0.3185  loss_rpn_cls: 0.01461  loss_rpn_loc: 0.01564  time: 0.6867  data_time: 0.0060  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:24:11 d2.utils.events]: \u001b[0m eta: 0:09:35  iter: 14159  total_loss: 0.8757  loss_cls: 0.1397  loss_box_reg: 0.4246  loss_mask: 0.2771  loss_rpn_cls: 0.01634  loss_rpn_loc: 0.01776  time: 0.6865  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:24:25 d2.utils.events]: \u001b[0m eta: 0:09:21  iter: 14179  total_loss: 0.9075  loss_cls: 0.177  loss_box_reg: 0.4017  loss_mask: 0.3085  loss_rpn_cls: 0.01274  loss_rpn_loc: 0.02266  time: 0.6865  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:24:38 d2.utils.events]: \u001b[0m eta: 0:09:08  iter: 14199  total_loss: 0.9264  loss_cls: 0.1697  loss_box_reg: 0.4448  loss_mask: 0.2913  loss_rpn_cls: 0.01311  loss_rpn_loc: 0.0179  time: 0.6865  data_time: 0.0064  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:24:52 d2.utils.events]: \u001b[0m eta: 0:08:54  iter: 14219  total_loss: 1.045  loss_cls: 0.191  loss_box_reg: 0.4435  loss_mask: 0.344  loss_rpn_cls: 0.01795  loss_rpn_loc: 0.02324  time: 0.6865  data_time: 0.0070  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:25:05 d2.utils.events]: \u001b[0m eta: 0:08:40  iter: 14239  total_loss: 0.7531  loss_cls: 0.1265  loss_box_reg: 0.3314  loss_mask: 0.2721  loss_rpn_cls: 0.009817  loss_rpn_loc: 0.01645  time: 0.6864  data_time: 0.0049  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:25:19 d2.utils.events]: \u001b[0m eta: 0:08:26  iter: 14259  total_loss: 0.9242  loss_cls: 0.1394  loss_box_reg: 0.4406  loss_mask: 0.2969  loss_rpn_cls: 0.01551  loss_rpn_loc: 0.01874  time: 0.6863  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:25:32 d2.utils.events]: \u001b[0m eta: 0:08:11  iter: 14279  total_loss: 0.7482  loss_cls: 0.1343  loss_box_reg: 0.352  loss_mask: 0.2521  loss_rpn_cls: 0.01391  loss_rpn_loc: 0.02015  time: 0.6861  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:25:45 d2.utils.events]: \u001b[0m eta: 0:07:58  iter: 14299  total_loss: 0.8627  loss_cls: 0.1352  loss_box_reg: 0.4417  loss_mask: 0.2954  loss_rpn_cls: 0.01275  loss_rpn_loc: 0.01269  time: 0.6861  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:26:00 d2.utils.events]: \u001b[0m eta: 0:07:45  iter: 14319  total_loss: 0.9294  loss_cls: 0.175  loss_box_reg: 0.4147  loss_mask: 0.2998  loss_rpn_cls: 0.01228  loss_rpn_loc: 0.01908  time: 0.6862  data_time: 0.0051  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:26:14 d2.utils.events]: \u001b[0m eta: 0:07:31  iter: 14339  total_loss: 1.038  loss_cls: 0.1845  loss_box_reg: 0.4105  loss_mask: 0.3167  loss_rpn_cls: 0.01707  loss_rpn_loc: 0.02513  time: 0.6863  data_time: 0.0061  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:26:28 d2.utils.events]: \u001b[0m eta: 0:07:18  iter: 14359  total_loss: 0.8784  loss_cls: 0.1526  loss_box_reg: 0.384  loss_mask: 0.2844  loss_rpn_cls: 0.009219  loss_rpn_loc: 0.01747  time: 0.6864  data_time: 0.0065  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:26:42 d2.utils.events]: \u001b[0m eta: 0:07:03  iter: 14379  total_loss: 0.7933  loss_cls: 0.1232  loss_box_reg: 0.3752  loss_mask: 0.2625  loss_rpn_cls: 0.01114  loss_rpn_loc: 0.01259  time: 0.6864  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:26:55 d2.utils.events]: \u001b[0m eta: 0:06:49  iter: 14399  total_loss: 0.7374  loss_cls: 0.1074  loss_box_reg: 0.2873  loss_mask: 0.2893  loss_rpn_cls: 0.01913  loss_rpn_loc: 0.0126  time: 0.6863  data_time: 0.0054  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:27:08 d2.utils.events]: \u001b[0m eta: 0:06:35  iter: 14419  total_loss: 0.7874  loss_cls: 0.1261  loss_box_reg: 0.3552  loss_mask: 0.2445  loss_rpn_cls: 0.009505  loss_rpn_loc: 0.01524  time: 0.6861  data_time: 0.0066  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:27:22 d2.utils.events]: \u001b[0m eta: 0:06:22  iter: 14439  total_loss: 0.7984  loss_cls: 0.1498  loss_box_reg: 0.3694  loss_mask: 0.2634  loss_rpn_cls: 0.01238  loss_rpn_loc: 0.01441  time: 0.6862  data_time: 0.0058  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:27:36 d2.utils.events]: \u001b[0m eta: 0:06:09  iter: 14459  total_loss: 0.9329  loss_cls: 0.1454  loss_box_reg: 0.4361  loss_mask: 0.2861  loss_rpn_cls: 0.01433  loss_rpn_loc: 0.01882  time: 0.6862  data_time: 0.0060  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:27:49 d2.utils.events]: \u001b[0m eta: 0:05:54  iter: 14479  total_loss: 0.9777  loss_cls: 0.1679  loss_box_reg: 0.4548  loss_mask: 0.2727  loss_rpn_cls: 0.01554  loss_rpn_loc: 0.0234  time: 0.6861  data_time: 0.0053  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:28:03 d2.utils.events]: \u001b[0m eta: 0:05:41  iter: 14499  total_loss: 0.8569  loss_cls: 0.1372  loss_box_reg: 0.4026  loss_mask: 0.2891  loss_rpn_cls: 0.01373  loss_rpn_loc: 0.0169  time: 0.6861  data_time: 0.0058  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:28:16 d2.utils.events]: \u001b[0m eta: 0:05:27  iter: 14519  total_loss: 0.9312  loss_cls: 0.1579  loss_box_reg: 0.431  loss_mask: 0.2717  loss_rpn_cls: 0.01489  loss_rpn_loc: 0.01868  time: 0.6861  data_time: 0.0067  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:28:30 d2.utils.events]: \u001b[0m eta: 0:05:14  iter: 14539  total_loss: 0.8511  loss_cls: 0.1469  loss_box_reg: 0.3852  loss_mask: 0.2892  loss_rpn_cls: 0.01099  loss_rpn_loc: 0.021  time: 0.6861  data_time: 0.0060  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:28:44 d2.utils.events]: \u001b[0m eta: 0:05:00  iter: 14559  total_loss: 0.7894  loss_cls: 0.1595  loss_box_reg: 0.3789  loss_mask: 0.2118  loss_rpn_cls: 0.009509  loss_rpn_loc: 0.01495  time: 0.6862  data_time: 0.0061  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:28:57 d2.utils.events]: \u001b[0m eta: 0:04:46  iter: 14579  total_loss: 0.9173  loss_cls: 0.1441  loss_box_reg: 0.3928  loss_mask: 0.3307  loss_rpn_cls: 0.01944  loss_rpn_loc: 0.02861  time: 0.6860  data_time: 0.0070  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:29:11 d2.utils.events]: \u001b[0m eta: 0:04:32  iter: 14599  total_loss: 1.009  loss_cls: 0.1863  loss_box_reg: 0.4408  loss_mask: 0.3398  loss_rpn_cls: 0.02046  loss_rpn_loc: 0.0187  time: 0.6860  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:29:25 d2.utils.events]: \u001b[0m eta: 0:04:19  iter: 14619  total_loss: 1.189  loss_cls: 0.2616  loss_box_reg: 0.5707  loss_mask: 0.3394  loss_rpn_cls: 0.02  loss_rpn_loc: 0.02178  time: 0.6862  data_time: 0.0064  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:29:39 d2.utils.events]: \u001b[0m eta: 0:04:05  iter: 14639  total_loss: 0.9518  loss_cls: 0.1613  loss_box_reg: 0.3965  loss_mask: 0.3211  loss_rpn_cls: 0.01754  loss_rpn_loc: 0.01767  time: 0.6861  data_time: 0.0075  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:29:52 d2.utils.events]: \u001b[0m eta: 0:03:51  iter: 14659  total_loss: 0.8799  loss_cls: 0.1656  loss_box_reg: 0.406  loss_mask: 0.3074  loss_rpn_cls: 0.01948  loss_rpn_loc: 0.02371  time: 0.6861  data_time: 0.0056  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:30:06 d2.utils.events]: \u001b[0m eta: 0:03:37  iter: 14679  total_loss: 0.88  loss_cls: 0.1454  loss_box_reg: 0.4356  loss_mask: 0.2741  loss_rpn_cls: 0.01519  loss_rpn_loc: 0.01847  time: 0.6860  data_time: 0.0057  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:30:19 d2.utils.events]: \u001b[0m eta: 0:03:23  iter: 14699  total_loss: 0.9665  loss_cls: 0.1884  loss_box_reg: 0.4091  loss_mask: 0.3162  loss_rpn_cls: 0.01832  loss_rpn_loc: 0.02416  time: 0.6860  data_time: 0.0057  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:30:33 d2.utils.events]: \u001b[0m eta: 0:03:09  iter: 14719  total_loss: 1.085  loss_cls: 0.2236  loss_box_reg: 0.4834  loss_mask: 0.3206  loss_rpn_cls: 0.02099  loss_rpn_loc: 0.02172  time: 0.6859  data_time: 0.0065  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:30:46 d2.utils.events]: \u001b[0m eta: 0:02:56  iter: 14739  total_loss: 0.7972  loss_cls: 0.1322  loss_box_reg: 0.3566  loss_mask: 0.2562  loss_rpn_cls: 0.009554  loss_rpn_loc: 0.01535  time: 0.6858  data_time: 0.0064  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:31:00 d2.utils.events]: \u001b[0m eta: 0:02:41  iter: 14759  total_loss: 0.9429  loss_cls: 0.1816  loss_box_reg: 0.4241  loss_mask: 0.2806  loss_rpn_cls: 0.01535  loss_rpn_loc: 0.01676  time: 0.6858  data_time: 0.0062  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:31:14 d2.utils.events]: \u001b[0m eta: 0:02:29  iter: 14779  total_loss: 0.8916  loss_cls: 0.107  loss_box_reg: 0.4213  loss_mask: 0.306  loss_rpn_cls: 0.009801  loss_rpn_loc: 0.01669  time: 0.6859  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:31:29 d2.utils.events]: \u001b[0m eta: 0:02:15  iter: 14799  total_loss: 0.9034  loss_cls: 0.1503  loss_box_reg: 0.4193  loss_mask: 0.2837  loss_rpn_cls: 0.01416  loss_rpn_loc: 0.02388  time: 0.6862  data_time: 0.0062  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:31:43 d2.utils.events]: \u001b[0m eta: 0:02:01  iter: 14819  total_loss: 1.027  loss_cls: 0.1572  loss_box_reg: 0.432  loss_mask: 0.3299  loss_rpn_cls: 0.01705  loss_rpn_loc: 0.02803  time: 0.6862  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:31:56 d2.utils.events]: \u001b[0m eta: 0:01:48  iter: 14839  total_loss: 0.9129  loss_cls: 0.172  loss_box_reg: 0.4375  loss_mask: 0.3232  loss_rpn_cls: 0.01954  loss_rpn_loc: 0.02039  time: 0.6861  data_time: 0.0054  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:32:10 d2.utils.events]: \u001b[0m eta: 0:01:34  iter: 14859  total_loss: 0.6222  loss_cls: 0.1269  loss_box_reg: 0.2765  loss_mask: 0.2085  loss_rpn_cls: 0.01336  loss_rpn_loc: 0.01215  time: 0.6860  data_time: 0.0062  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:32:23 d2.utils.events]: \u001b[0m eta: 0:01:21  iter: 14879  total_loss: 1.037  loss_cls: 0.1782  loss_box_reg: 0.4903  loss_mask: 0.3181  loss_rpn_cls: 0.01741  loss_rpn_loc: 0.01871  time: 0.6860  data_time: 0.0069  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:32:37 d2.utils.events]: \u001b[0m eta: 0:01:07  iter: 14899  total_loss: 0.9863  loss_cls: 0.1874  loss_box_reg: 0.4632  loss_mask: 0.2812  loss_rpn_cls: 0.01113  loss_rpn_loc: 0.01984  time: 0.6860  data_time: 0.0070  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:32:51 d2.utils.events]: \u001b[0m eta: 0:00:54  iter: 14919  total_loss: 0.9654  loss_cls: 0.1483  loss_box_reg: 0.4087  loss_mask: 0.293  loss_rpn_cls: 0.01718  loss_rpn_loc: 0.01552  time: 0.6860  data_time: 0.0065  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:33:04 d2.utils.events]: \u001b[0m eta: 0:00:40  iter: 14939  total_loss: 0.7768  loss_cls: 0.1224  loss_box_reg: 0.3626  loss_mask: 0.2698  loss_rpn_cls: 0.01474  loss_rpn_loc: 0.01561  time: 0.6860  data_time: 0.0067  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:33:18 d2.utils.events]: \u001b[0m eta: 0:00:27  iter: 14959  total_loss: 0.914  loss_cls: 0.1574  loss_box_reg: 0.4245  loss_mask: 0.3054  loss_rpn_cls: 0.01413  loss_rpn_loc: 0.0165  time: 0.6859  data_time: 0.0063  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:33:31 d2.utils.events]: \u001b[0m eta: 0:00:13  iter: 14979  total_loss: 1.101  loss_cls: 0.1826  loss_box_reg: 0.4939  loss_mask: 0.3048  loss_rpn_cls: 0.01339  loss_rpn_loc: 0.01957  time: 0.6859  data_time: 0.0062  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:33:50 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 14999  total_loss: 1.018  loss_cls: 0.1696  loss_box_reg: 0.4619  loss_mask: 0.307  loss_rpn_cls: 0.01552  loss_rpn_loc: 0.022  time: 0.6859  data_time: 0.0072  lr: 0.0001  max_mem: 4305M\n",
            "\u001b[32m[12/02 15:33:50 d2.engine.hooks]: \u001b[0mOverall training speed: 4998 iterations in 0:57:08 (0.6859 s / it)\n",
            "\u001b[32m[12/02 15:33:50 d2.engine.hooks]: \u001b[0mTotal training time: 0:57:17 (0:00:09 on hooks)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXibxbmuxCNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11369bc0-e190-4446-9fa4-97159b15ffad"
      },
      "source": [
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from itertools import groupby\n",
        "from pycocotools import mask as maskutil\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "import json\n",
        "setup_logger()\n",
        "\n",
        "def binary_mask_to_rle(binary_mask):\n",
        "    rle = {'counts': [], 'size': list(binary_mask.shape)}\n",
        "    counts = rle.get('counts')\n",
        "    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order='F'))):\n",
        "        if i == 0 and value == 1:\n",
        "            counts.append(0)\n",
        "        counts.append(len(list(elements)))\n",
        "    compressed_rle = maskutil.frPyObjects(rle, rle.get('size')[0], rle.get('size')[1])\n",
        "    compressed_rle['counts'] = str(compressed_rle['counts'], encoding='utf-8')\n",
        "    return compressed_rle\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"./configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")\n",
        "cfg.DATASETS.TRAIN = (\"Tiny_dataset\", )\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.SOLVER.MAX_ITER = 500\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 20 \n",
        "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/DL_HW3/detectron2_repo/output/model_final.pth\"\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.55\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "cocoGt = COCO(\"/content/drive/MyDrive/DL_HW3/test.json\")\n",
        "\n",
        "\n",
        "coco_dt = []\n",
        "count=0\n",
        "\n",
        "for imgid in cocoGt.imgs:\n",
        "    image = cv2.imread(\"/content/drive/MyDrive/DL_HW3/test_images/\" + cocoGt.loadImgs(ids=imgid)[0]['file_name'])[:,:,::-1] \n",
        "    result = predictor(image) # run inference\n",
        "    for i in range(len(result[\"instances\"].scores)):\n",
        "        pred = {}\n",
        "        pred['image_id'] = imgid\n",
        "        pred['category_id'] = int(result[\"instances\"].pred_classes[i] + 1)\n",
        "        binary_mask = result[\"instances\"].pred_masks[i].to(\"cpu\").numpy()\n",
        "        pred['segmentation'] = binary_mask_to_rle(binary_mask) # save binary mask to RLE\n",
        "        pred['score'] = float(result[\"instances\"].scores[i])\n",
        "        coco_dt.append(pred)\n",
        "    \n",
        "    # Visualize the inference result\n",
        "    if(count<10):\n",
        "      v = Visualizer(image,\n",
        "                metadata=Tiny_metadata,\n",
        "                scale=1.2, \n",
        "                instance_mode=ColorMode.IMAGE_BW  \n",
        "      )\n",
        "      v = v.draw_instance_predictions(result[\"instances\"].to(\"cpu\"))\n",
        "      cv2_imshow(v.get_image()[:,:,::-1])\n",
        "\n",
        "    count+=1\n",
        "    \n",
        "\n",
        "with open(\"/content/drive/MyDrive/DL_HW3/detectron2_repo/output/309553007.json\", \"w\") as f:\n",
        "    json.dump(coco_dt, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.34s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DL_HW3/detectron2_repo/detectron2/modeling/roi_heads/fast_rcnn.py:124: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  filter_inds = filter_mask.nonzero()\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}